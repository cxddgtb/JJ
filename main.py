# ================================================================
#                Project Prometheus - Final Prophet Version
#              (Future Prediction Chart & All Previous Features)
# ================================================================
import os
import sys
import json
import yaml
import logging
from datetime import datetime, timedelta
import pytz
import pandas as pd
import akshare as ak
import pandas_ta as ta
from fredapi import Fred
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
from concurrent.futures import ThreadPoolExecutor
from tenacity import retry, stop_after_attempt, wait_fixed
import google.generativeai as genai
import requests
from bs4 import BeautifulSoup
import google.api_core.exceptions
import markdown
import ffn # <--- å¯¼å…¥æ–°åº“
import bt  # <--- å¯¼å…¥æ–°åº“

# --- Section 1: Setup & Configuration ---
try:
    with open('config.yaml', 'r', encoding='utf-8') as f: config = yaml.safe_load(f)
except FileNotFoundError: print("FATAL: config.yaml not found. Exiting."); sys.exit(1)
LOG_DIR = 'logs'; CHART_DIR = 'charts'
os.makedirs(LOG_DIR, exist_ok=True); os.makedirs(CHART_DIR, exist_ok=True)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [%(levelname)s] - %(message)s',
                    handlers=[logging.FileHandler(os.path.join(LOG_DIR, 'workflow.log'), mode='w'), logging.StreamHandler()])
matplotlib.use('Agg'); matplotlib.rcParams['font.sans-serif'] = ['WenQuanYi Zen Hei']; matplotlib.rcParams['axes.unicode_minus'] = False
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
if not GEMINI_API_KEY: logging.error("FATAL: GEMINI_API_KEY environment variable not set."); sys.exit(1)
genai.configure(api_key=GEMINI_API_KEY); AI_MODEL = genai.GenerativeModel('gemini-1.5-pro-latest')

# --- Section 2: Data Acquisition & Future Prediction Module ---
# ... (scrape_news, fetch_historical_data_akshare, get_economic_data remain the same) ...
@retry(stop=stop_after_attempt(3), wait=wait_fixed(3))
def fetch_url_content_raw(url):
    headers = {'User-Agent': 'Mozilla/5.0 ...'}; response = requests.get(url, headers=headers, timeout=20)
    response.raise_for_status(); return response.content
def scrape_news():
    headlines = []
    for source in config['data_sources']['news_urls']:
        try:
            raw_html = fetch_url_content_raw(source['url']); soup = BeautifulSoup(raw_html, 'html.parser')
            for link in soup.find_all('a', href=True):
                if len(link.text.strip()) > 20 and '...' not in link.text: headlines.append(link.text.strip())
        except Exception as e: logging.error(f"ä» {source['name']} çˆ¬å–æ–°é—»å¤±è´¥: {e}")
    return list(set(headlines))[:15]
@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))
def fetch_historical_data_akshare(code, days):
    df = ak.fund_etf_hist_em(symbol=code, period="daily", adjust="qfq")
    if df.empty: raise ValueError(f"AKShareæœªèƒ½è·å–åˆ°ä»£ç  {code} çš„æ•°æ®ã€‚")
    df = df.rename(columns={'æ—¥æœŸ': 'Date', 'å¼€ç›˜': 'Open', 'æ”¶ç›˜': 'Close', 'æœ€é«˜': 'High', 'æœ€ä½': 'Low', 'æˆäº¤é‡': 'Volume'})
    df['Date'] = pd.to_datetime(df['Date']); df = df.set_index('Date')
    cols_to_numeric = ['Open', 'Close', 'High', 'Low', 'Volume']
    df[cols_to_numeric] = df[cols_to_numeric].apply(pd.to_numeric, errors='coerce'); df.dropna(subset=cols_to_numeric, inplace=True)
    df = df[df.index >= (datetime.now() - timedelta(days=days))]; df = df.sort_index(); df['code'] = code; return df
def get_economic_data():
    if not config['economic_data']['enabled']: return "å®è§‚ç»æµæ•°æ®æ¨¡å—å·²ç¦ç”¨ã€‚"
    try:
        fred_key = os.getenv('FRED_API_KEY'); fred = Fred(api_key=fred_key)
        data_points = {indicator: f"{fred.get_series(indicator).iloc[-1]} (æˆªè‡³ {fred.get_series(indicator).index[-1].strftime('%Y-%m-%d')})" for indicator in config['economic_data']['indicators']}
        return f"æœ€æ–°å®è§‚ç»æµæŒ‡æ ‡: {json.dumps(data_points, indent=2, ensure_ascii=False)}"
    except Exception as e: logging.error(f"è·å–FREDæ•°æ®å¤±è´¥: {e}"); return "æ— æ³•æ£€ç´¢å®è§‚ç»æµæ•°æ®ã€‚"

# --- NEW: Function to generate future prediction chart for a SINGLE fund ---
def generate_future_trend_chart(fund_code, data):
    """
    Generates a mini prediction chart for the next 30 days based on historical volatility.
    """
    try:
        # 1. Calculate historical daily returns and volatility
        returns = data['Close'].pct_change().dropna()
        if len(returns) < 30: # Need at least some data
            return None
        
        # 2. Setup simulation parameters
        last_price = data['Close'].iloc[-1]
        num_simulations = 100
        num_days = 30
        
        # 3. Run Monte Carlo simulation
        simulation_df = pd.DataFrame()
        for x in range(num_simulations):
            daily_vol = returns.std()
            price_series = [last_price]
            
            # Generate random returns for 30 days
            price = last_price * (1 + np.random.normal(0, daily_vol))
            price_series.append(price)
            for _ in range(num_days -1):
                price = price_series[-1] * (1 + np.random.normal(0, daily_vol))
                price_series.append(price)
            
            simulation_df[x] = price_series
        
        # 4. Create the mini chart
        plt.figure(figsize=(3, 1)) # Small size for embedding in table
        # Plot all simulations with high transparency
        for i in range(num_simulations):
            plt.plot(simulation_df.iloc[:, i], color='grey', alpha=0.1)
        
        # Plot the median path (50th percentile) more prominently
        median_path = simulation_df.quantile(0.5, axis=1)
        plt.plot(median_path, color='blue', linewidth=2)
        
        # Clean up the chart for a "K-line" feel
        plt.axis('off')
        plt.margins(0)
        plt.tick_params(axis='both', left=False, top=False, right=False, bottom=False, labelleft=False, labeltop=False, labelright=False, labelbottom=False)
        
        chart_path = os.path.join(CHART_DIR, f"{fund_code}_future_trend.png")
        plt.savefig(chart_path, bbox_inches='tight', pad_inches=0, dpi=50)
        plt.close()
        
        return chart_path
    except Exception as e:
        logging.error(f"ä¸ºåŸºé‡‘ {fund_code} ç”Ÿæˆæœªæ¥è¶‹åŠ¿å›¾å¤±è´¥: {e}")
        return None

# --- Section 5A: Template-Based Fallback Report (Upgraded with Future Chart) ---
def generate_template_report(context):
    logging.warning("AI APIé…é¢è€—å°½ï¼Œåˆ‡æ¢åˆ°Bè®¡åˆ’ï¼šæ¨¡æ¿åŒ–æ•°æ®æŠ¥å‘Šã€‚")
    
    # --- NEW: Upgraded table with future prediction chart ---
    quant_table = "| åŸºé‡‘åç§° | çŠ¶æ€ | RSI(14) | MACDä¿¡å· | æœªæ¥30æ—¥è¶‹åŠ¿é¢„æµ‹ |\n| :--- | :--- | :--- | :--- | :--- |\n"
    for item in context.get('quant_analysis_data', []):
        chart_md = 'N/A'
        if 'future_chart_path' in item and item['future_chart_path']:
             # Use a relative path for Markdown
            chart_md = f"![è¶‹åŠ¿å›¾]({item['future_chart_path']})"
        quant_table += f"| {item['name']} ({item['code']}) | {item['status']} | {item.get('rsi', 'N/A')} | {item.get('macd', 'N/A')} | {chart_md} |\n"
    
    news_section = "### å¸‚åœºæ–°é—»æ‘˜è¦\n"
    news_list = context.get('news', []); news_section += "\n- " + "\n- ".join(news_list) if news_list else "æœªèƒ½æˆåŠŸè·å–æœ€æ–°æ–°é—»ã€‚"
    summary_report = f"# âš ï¸ æ™®ç½—ç±³ä¿®æ–¯æ•°æ®ç®€æŠ¥ (AIåˆ†æå¤±è´¥)\n**æŠ¥å‘Šæ—¶é—´:** {context['current_time']}\n**è­¦å‘Š:** å› Gemini APIé…é¢è€—å°½ï¼Œä»Šæ—¥æœªç”Ÿæˆæ™ºèƒ½åˆ†æã€‚ä»¥ä¸‹ä¸ºåŸå§‹æ•°æ®æ‘˜è¦ã€‚\n---\n### é‡åŒ–æŒ‡æ ‡ä¸è¶‹åŠ¿é¢„æµ‹\n{quant_table}\n---\n{news_section}\n---\n*æç¤ºï¼šè¦æ¢å¤AIæ™ºèƒ½åˆ†æï¼Œè¯·ä¸ºæ‚¨çš„Google Cloudé¡¹ç›®å¯ç”¨ç»“ç®—ã€‚*"
    return summary_report.strip(), "å› AI APIé…é¢è€—å°½ï¼Œæœªç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Šã€‚"

# --- Section 5B: Ultimate AI Council ---
def ultimate_ai_council(context):
    logging.info("æ­£åœ¨å¬å¼€Aè®¡åˆ’ï¼šç»ˆæAIå§”å‘˜ä¼š...")
    # The AI doesn't need to see the chart, just the raw data. The prompt remains the same.
    quant_analysis_for_ai = "æœ€æ–°æŠ€æœ¯æŒ‡æ ‡åˆ†æ:\n"
    for item in context.get('quant_analysis_data', []):
        quant_analysis_for_ai += f"  - **{item['name']} ({item['code']})**: {item['status']}ã€‚RSI={item.get('rsi', 'N/A')}, MACDä¿¡å·={item.get('macd', 'N/A')}\n"
    
    prompt = f"""
    æ‚¨æ˜¯â€œæ™®ç½—ç±³ä¿®æ–¯â€AIï¼Œ... (rest of the prompt is the same as the last version) ...
    **3. é‡åŒ–åˆ†æ (æ•°æ®ã€æŒ‡æ ‡):**
    {quant_analysis_for_ai}
    ...
    ### æŠ•èµ„ç»„åˆä»ªè¡¨ç›˜
    | åŸºé‡‘åç§° | ç±»å‹ | **æ“ä½œå»ºè®®** | **ä¿¡å¿ƒæŒ‡æ•°** | æ ¸å¿ƒç†ç”± |
    | :--- | :--- | :--- | :--- | :--- |
    ...
    """
    try:
        logging.info("æ­£åœ¨ä½¿ç”¨ Gemini 1.5 Pro ç”Ÿæˆä¸­æ–‡æŠ¥å‘Š...")
        response = AI_MODEL.generate_content(prompt)
        report_text = response.text
        if "---DETAILED_REPORT_CUT---" in report_text: summary, detail = report_text.split("---DETAILED_REPORT_CUT---", 1)
        else: summary, detail = report_text, "AIæœªèƒ½ç”Ÿæˆç‹¬ç«‹çš„è¯¦ç»†æŠ¥å‘Šã€‚"
        return summary.strip(), detail.strip()
    except google.api_core.exceptions.ResourceExhausted as e:
        logging.error(f"AIæŠ¥å‘Šå¤±è´¥ï¼ŒAPIé…é¢è€—å°½: {e}"); return generate_template_report(context)
    except Exception as e:
        logging.error(f"AIæŠ¥å‘ŠæœªçŸ¥é”™è¯¯: {e}"); summary, detail = generate_template_report(context)
        return f"# ğŸ”¥ AIåˆ†æé­é‡æœªçŸ¥é”™è¯¯\n\n{summary}", detail

# --- Section 6: Main Execution Block (Upgraded with Future Chart) ---
def main():
    start_time = datetime.now(pytz.timezone('Asia/Shanghai')); logging.info(f"--- æ™®ç½—ç±³ä¿®æ–¯å¼•æ“å¯åŠ¨äº {start_time.strftime('%Y-%m-%d %H:%M:%S')} (AKShareæ ¸å¿ƒ) ---")
    context = {'current_time': start_time.strftime('%Y-%m-%d %H:%M:%S %Z'), 'current_date': start_time.strftime('%Y-%m-%d')}
    with ThreadPoolExecutor(max_workers=10) as executor:
        news_future, eco_future = executor.submit(scrape_news), executor.submit(get_economic_data)
        fund_codes = [f['code'] for f in config['index_funds']]
        hist_data_futures = {code: executor.submit(fetch_historical_data_akshare, code, 365) for code in fund_codes}
        context['news'], context['economic_data'] = news_future.result(), eco_future.result()
        
        all_fund_data, quant_data_structured = {}, []
        for code in fund_codes:
            future = hist_data_futures[code]; fund_name = next((f['name'] for f in config['index_funds'] if f['code'] == code), code)
            item = {'name': fund_name, 'code': code}
            try:
                data = future.result(); all_fund_data[code] = data
                data.ta.rsi(append=True); data.ta.macd(append=True)
                latest = data.iloc[-1]
                macd_signal = 'é‡‘å‰' if latest['MACD_12_26_9'] > latest['MACDs_12_26_9'] else 'æ­»å‰'
                
                # --- NEW: Generate future trend chart for each fund ---
                future_chart_path = generate_future_trend_chart(code, data)
                
                item.update({'status': 'æ•°æ®æ­£å¸¸', 'rsi': f"{latest['RSI_14']:.2f}", 'macd': macd_signal, 'future_chart_path': future_chart_path})
            except Exception as e:
                logging.error(f"å¤„ç†åŸºé‡‘ {fund_name} ({code}) çš„æ•°æ®å¤±è´¥: {e}"); item.update({'status': 'æ•°æ®è·å–å¤±è´¥'})
            quant_data_structured.append(item)
        context['quant_analysis_data'] = quant_data_structured
    
    try:
        with open(config['user_profile']['portfolio_path'], 'r', encoding='utf-8') as f: context['portfolio'] = json.load(f)
    except Exception as e: logging.error(f"æ— æ³•åŠ è½½æŒä»“æ–‡ä»¶: {e}"); context['portfolio'] = [{"é”™è¯¯": "æ— æ³•åŠ è½½æŒä»“æ–‡ä»¶ã€‚"}]

    if not all_fund_data: summary_report, detail_report = (f"# ğŸ”¥ ç®€æŠ¥ç”Ÿæˆå¤±è´¥ï¼šæ— æœ‰æ•ˆæ•°æ®\n\næ‰€æœ‰ç›®æ ‡åŸºé‡‘çš„æ•°æ®è·å–å‡å¤±è´¥ã€‚", "è¯·æ£€æŸ¥æ—¥å¿—ã€‚")
    else: summary_report, detail_report = ultimate_ai_council(context)
    
    with open("README.md", "w", encoding="utf-8") as f: f.write(summary_report)
    report_filename = f"reports/report_{context['current_date']}.md"; os.makedirs('reports', exist_ok=True)
    with open(report_filename, "w", encoding='utf-8') as f: f.write(detail_report)
    
    end_time = datetime.now(pytz.timezone('Asia/Shanghai')); logging.info(f"--- æ™®ç½—ç±³ä¿®æ–¯å¼•æ“ç»“æŸäº {end_time.strftime('%Y-%m-%d %H:%M:%S')} ---")
    logging.info(f"--- æ€»è¿è¡Œæ—¶é—´: {end_time - start_time} ---")

if __name__ == "__main__":
    main()
