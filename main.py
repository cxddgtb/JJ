import os
import datetime
import time
import json
import re
import concurrent.futures
import traceback
import google.generativeai as genai
import requests
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
from ddgs import DDGS
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# --- å…¨å±€é…ç½® ---
HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}
TIMESTAMP_FILE, MIN_INTERVAL_HOURS, REQUESTS_TIMEOUT = "last_run_timestamp.txt", 6, 30

# --- åŸºç¡€è¾…åŠ©å‡½æ•° (æ— æ”¹åŠ¨) ---
def load_config():
    with open('config.json', 'r', encoding='utf-8') as f: config = json.load(f)
    config.setdefault('historical_days_to_display', 7)
    return config

def check_time_interval():
    github_event_name = os.getenv('GITHUB_EVENT_NAME')
    if github_event_name != 'repository_dispatch': return True
    if not os.path.exists(TIMESTAMP_FILE): return True
    with open(TIMESTAMP_FILE, "r") as f: last_run_timestamp = float(f.read())
    hours_diff = (time.time() - last_run_timestamp) / 3600
    if hours_diff < MIN_INTERVAL_HOURS: return False
    return True

def update_timestamp():
    with open(TIMESTAMP_FILE, "w") as f: f.write(str(time.time()))

# --- æ•°æ®è·å–æ¨¡å— (â€œä¸‰å±‚æƒ…æŠ¥ç½‘ç»œâ€) ---

# --- ç¬¬ä¸€å±‚ï¼šå¸¸è§„éƒ¨é˜Ÿ (API) ---
def get_from_eastmoney_api(fund_code, history_days):
    url = f"http://api.fund.eastmoney.com/f10/lsjz?fundCode={fund_code}&pageIndex=1&pageSize={history_days}"
    response = requests.get(url, headers=HEADERS, timeout=REQUESTS_TIMEOUT)
    response.raise_for_status()
    data = response.json().get('Data')
    if not data or not data.get('LSJZList'): raise ValueError("APIæ•°æ®æ— æ•ˆ")
    return data

# --- ç¬¬äºŒå±‚ï¼šç‰¹ç§éƒ¨é˜Ÿ (æµè§ˆå™¨æ¨¡æ‹Ÿ) ---
def get_from_sina_web(fund_code, history_days):
    options = webdriver.ChromeOptions(); options.add_argument("--headless"); options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage"); options.add_argument("user-agent=" + HEADERS['User-Agent'])
    service = Service(ChromeDriverManager().install()); driver = webdriver.Chrome(service=service, options=options)
    try:
        url = f"http://money.finance.sina.com.cn/fund/hgsz/{fund_code}.html"
        driver.get(url)
        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, "fund_history_table")))
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        rows = soup.find('table', id='fund_history_table').find('tbody').find_all('tr')
        lsjz_list = [{'FSRQ': c[0].text, 'DWJZ': c[1].text, 'JZZZL': c[3].text.replace('%','')} for r in rows[:history_days] if len(c:=r.find_all('td'))>3]
        fund_name = soup.find('h1', id='fund_name').text.split('(')[0].strip()
        lsjz_list.reverse()
        return {'FundBaseInfo': {'JJJC': fund_name}, 'LSJZList': lsjz_list}
    finally: driver.quit()

# --- ç¬¬ä¸‰å±‚ï¼šç»ˆææƒ…æŠ¥å‘˜ (æœç´¢å¼•æ“) ---
def get_from_search_engine(fund_code):
    query = f"{fund_code} åŸºé‡‘å‡€å€¼"
    with DDGS() as ddgs:
        results = list(ddgs.text(query, region='cn-zh', max_results=3))
        if not results: raise ValueError("æœç´¢å¼•æ“æœªè¿”å›ç»“æœ")
        # å°è¯•ä»æœç´¢ç»“æœæ‘˜è¦ä¸­ç”¨æ­£åˆ™è¡¨è¾¾å¼è§£æ
        for result in results:
            snippet = result.get('body', '')
            # åŒ¹é… "å‡€å€¼" "æ—¥æœŸ" "æ¶¨å¹…" ç­‰å…³é”®è¯
            match = re.search(r'(\d{4}-\d{2}-\d{2}).*?å•ä½å‡€å€¼.*?(\d+\.\d+).*?æ—¥å¢é•¿ç‡.*?(-?\d+\.\d+)%', snippet)
            if match:
                fsrq, dwjz, jzzzl = match.groups()
                # æœç´¢å¼•æ“åªèƒ½è·å–æœ€æ–°ä¸€å¤©çš„æ•°æ®
                return {'FundBaseInfo': {'JJJC': result.get('title', fund_code)}, 'LSJZList': [{'FSRQ': fsrq, 'DWJZ': dwjz, 'JZZZL': jzzzl}]}
    raise ValueError("æ— æ³•ä»æœç´¢ç»“æœä¸­è§£æå‡€å€¼")

# --- â€œæƒ…æŠ¥æ±‡æ€»å®˜â€ ---
def get_fund_raw_data_final_robust(fund_code, history_days):
    print(f"\nå¼€å§‹å¯¹åŸºé‡‘ {fund_code} è¿›è¡Œä¸‰å±‚æƒ…æŠ¥ç½‘ç»œå¹¶è¡Œè·å–...")
    # å®šä¹‰æ‰€æœ‰æƒ…æŠ¥å‘˜å’Œä»–ä»¬çš„ä»»åŠ¡
    sources = {
        "SINA_WEB": (get_from_sina_web, [fund_code, history_days]),
        "EASTMONEY_API": (get_from_eastmoney_api, [fund_code, history_days]),
        "SEARCH_ENGINE": (get_from_search_engine, [fund_code])
    }
    
    successful_results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=len(sources)) as executor:
        future_to_source = {executor.submit(func, *args): name for name, (func, args) in sources.items()}
        for future in concurrent.futures.as_completed(future_to_source):
            source_name = future_to_source[future]
            try:
                data = future.result()
                if data and data.get('LSJZList'):
                    latest_date = data['LSJZList'][-1].get('FSRQ', '0000-00-00')
                    successful_results.append({'source': source_name, 'date': latest_date, 'data': data})
                    print(f"    âœ… {source_name}: è·å–æˆåŠŸ, æœ€æ–°æ—¥æœŸ: {latest_date}")
            except Exception as e:
                print(f"    âŒ {source_name}: è·å–å¤±è´¥: {e}")

    if not successful_results:
        print(f"æ‰€æœ‰æƒ…æŠ¥æ¥æºå‡æœªèƒ½è·å– {fund_code} çš„æ•°æ®ã€‚")
        return None

    # æƒ…æŠ¥èåˆï¼šé€‰æ‹©æœ€æ–°æ—¥æœŸçš„æ•°æ®ï¼Œå¹¶è¿›è¡Œèåˆ
    latest_date = max(r['date'] for r in successful_results)
    best_sources = [r for r in successful_results if r['date'] == latest_date]
    print(f"  ğŸ“Š æ‰¾åˆ°æœ€æ–°æ•°æ®æ—¥æœŸä¸º {latest_date} çš„æ¥æº: {[r['source'] for r in best_sources]}")
    
    # æ•°æ®èåˆï¼šå¦‚æœå¤šä¸ªæ¥æºéƒ½æœ‰æœ€æ–°æ•°æ®ï¼Œå–å¹³å‡å€¼
    final_data = best_sources[0]['data'] # ä»¥ç¬¬ä¸€ä¸ªä¸ºåŸºç¡€
    if len(best_sources) > 1:
        latest_net_values = [float(r['data']['LSJZList'][-1]['DWJZ']) for r in best_sources]
        avg_net_value = np.mean(latest_net_values)
        final_data['LSJZList'][-1]['DWJZ'] = str(avg_net_value)
        print(f"  ğŸ† æ•°æ®èåˆå®Œæˆï¼Œé‡‡ç”¨å¹³å‡å‡€å€¼: {avg_net_value:.4f}")

    return final_data
    
# --- å…¶ä»–æ‰€æœ‰å‡½æ•° (process_fund_data, report generation, etc.) ---
# è¿™ä¸€éƒ¨åˆ†å’Œä¸Šä¸Šç‰ˆå®Œå…¨ä¸€æ ·ï¼Œåªéœ€ç¡®ä¿ search_news çš„å¼•ç”¨å·²æ›´æ–°
def search_news(keyword):
    print(f"æ­£åœ¨æœç´¢æ–°é—»: {keyword}...")
    try:
        with DDGS() as ddgs:
            return "\n".join([f"- [æ ‡é¢˜] {r['title']}\n  [æ‘˜è¦] {r.get('body', 'æ— ')}\n" for r in ddgs.news(keyword, region='cn-zh', safesearch='off', max_results=5)])
    except Exception as e: return f"æœç´¢å…³é”®è¯ '{keyword}' å¤±è´¥: {e}"

def process_fund_data(raw_data, fund_code, ma_days, days_to_display):
    try:
        print(f"æ­£åœ¨å¤„ç†åŸºé‡‘ {fund_code} çš„æ•°æ®...")
        fund_name = raw_data.get('FundBaseInfo', {}).get('JJJC', fund_code)
        df = pd.DataFrame(raw_data['LSJZList'])
        df['FSRQ'] = pd.to_datetime(df['FSRQ'])
        df['DWJZ'] = pd.to_numeric(df['DWJZ'])
        df['JZZZL'] = pd.to_numeric(df.get('JZZZL', '0'), errors='coerce').fillna(0)
        df = df.sort_values('FSRQ')
        df[f'MA{ma_days}'] = df['DWJZ'].rolling(window=ma_days).mean()
        latest_data = df.iloc[-1]
        structured_data = {'name': fund_name, 'code': fund_code, 'latest_price': latest_data['DWJZ'], 'latest_ma': latest_data[f'MA{ma_days}'], 'daily_growth': latest_data['JZZZL'], 'ma_days': ma_days}
        recent_df = df.tail(days_to_display).sort_values('FSRQ', ascending=False)
        table_rows = [f"| {row['FSRQ'].strftime('%Y-%m-%d')} | {row['DWJZ']:.4f}   | {row[f'MA{ma_days}']:.4f if not pd.isna(row[f'MA{ma_days}']) else 'N/A'}    | {'ğŸ“ˆ' if row['DWJZ'] > row[f'MA{ma_days}'] else 'ğŸ“‰' if not pd.isna(row[f'MA{ma_days}']) else 'ğŸ¤”'}  |" for _, row in recent_df.iterrows()]
        formatted_string = f"### åŸºé‡‘: {fund_name} ({fund_code})\n- **æœ€æ–°å‡€å€¼**: {latest_data['DWJZ']:.4f} (æ—¥æœŸ: {latest_data['FSRQ'].strftime('%Y-%m-%d')})\n- **{ma_days}æ—¥å‡çº¿**: {latest_data[f'MA{ma_days}']:.4f if not pd.isna(latest_data[f'MA{ma_days}']) else 'æ•°æ®ä¸è¶³'}\n- **æŠ€æœ¯åˆ†æ**: å½“å‰å‡€å€¼åœ¨ {ma_days}æ—¥å‡çº¿**{'ä¹‹ä¸Š' if latest_data['DWJZ'] > latest_data[f'MA{ma_days}'] else 'ä¹‹ä¸‹'}**ã€‚\n- **æœ€è¿‘ {days_to_display} æ—¥è¯¦ç»†æ•°æ®**:\n| æ—¥æœŸ       | å•ä½å‡€å€¼ | {ma_days}æ—¥å‡çº¿ | è¶‹åŠ¿ |\n|:-----------|:---------|:------------|:-----|\n" + "\n".join(table_rows)
        return structured_data, formatted_string
    except Exception as e:
        print(f"âŒ å¤„ç†åŸºé‡‘ {fund_code} æ•°æ®æ—¶å‡ºé”™: {e}"); traceback.print_exc()
        return None, None

def get_sector_data():
    print("æ­£åœ¨çˆ¬å–è¡Œä¸šæ¿å—æ•°æ®...")
    try:
        response = requests.get("http://quote.eastmoney.com/center/boardlist.html#industry_board", headers=HEADERS, timeout=REQUESTS_TIMEOUT)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        sectors = [{'name': cols[1].find('a').text.strip(), 'change': float(cols[4].text.strip().replace('%', ''))} for row in soup.select('table#table_wrapper-table tbody tr') if len(cols := row.find_all('td')) > 5]
        sectors.sort(key=lambda x: x['change'], reverse=True)
        rising_str = "\n".join([f"  - {s['name']}: {s['change']:.2f}%" for s in sectors[:10]])
        falling_str = "\n".join([f"  - {s['name']}: {s['change']:.2f}%" for s in sectors[-10:]])
        return f"**ã€çƒ­é—¨ä¸Šæ¶¨æ¿å—ã€‘**\n{rising_str}\n\n**ã€çƒ­é—¨ä¸‹è·Œæ¿å—ã€‘**\n{falling_str}"
    except Exception as e:
        print(f"âŒ è¡Œä¸šæ¿å—æ•°æ®çˆ¬å–å¤±è´¥: {e}"); return "è¡Œä¸šæ¿å—æ•°æ®çˆ¬å–å¤±è´¥ã€‚"

def generate_rule_based_report(fund_datas, beijing_time):
    print("æ­£åœ¨ç”Ÿæˆâ€œè§„åˆ™å¤§è„‘â€åˆ†ææŠ¥å‘Š...")
    report_parts = [f"# åŸºé‡‘é‡åŒ–è§„åˆ™åˆ†ææŠ¥å‘Š ({beijing_time.strftime('%Y-%m-%d')})\n", "æœ¬æŠ¥å‘Šç”±é¢„è®¾é‡åŒ–è§„åˆ™ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒã€‚\n"]
    if not fund_datas:
        report_parts.append("### **æ³¨æ„ï¼šæ‰€æœ‰æ•°æ®æºå‡æœªèƒ½æˆåŠŸè·å–ä»»ä½•åŸºé‡‘æ•°æ®ï¼Œæ— æ³•ç”Ÿæˆåˆ†æã€‚**\nè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åå†è¯•ã€‚")
    else:
        for data in fund_datas:
            score, reasons = 0, []
            if not pd.isna(data['latest_ma']):
                if data['latest_price'] > data['latest_ma']: score += 2; reasons.append(f"å‡€å€¼({data['latest_price']:.4f})åœ¨{data['ma_days']}æ—¥å‡çº¿({data['latest_ma']:.4f})ä¹‹ä¸Š")
                else: score -= 2; reasons.append(f"å‡€å€¼({data['latest_price']:.4f})åœ¨{data['ma_days']}æ—¥å‡çº¿({data['latest_ma']:.4f})ä¹‹ä¸‹")
            if data['daily_growth'] > 0: score += 1; reasons.append(f"å½“æ—¥ä¸Šæ¶¨({data['daily_growth']:.2f}%)")
            else: score -= 1; reasons.append(f"å½“æ—¥ä¸‹è·Œ({data['daily_growth']:.2f}%)")
            if score == 3: conclusion = "å¼ºçƒˆçœ‹å¥½ ğŸš€"
            elif score == 1: conclusion = "è°¨æ…ä¹è§‚ ğŸ‘"
            elif score == -1: conclusion = "æ³¨æ„é£é™© âš ï¸"
            else: conclusion = "å»ºè®®å‡ä»“ ğŸ“‰"
            report_parts.append(f"### {data['name']} ({data['code']})\n- **é‡åŒ–è¯„åˆ†**: {score}\n- **ç»¼åˆç»“è®º**: {conclusion}\n- **è¯„åˆ†ä¾æ®**: {', '.join(reasons)}\n")
    report_parts.append("\n---\n**å…è´£å£°æ˜**: æœ¬æŠ¥å‘Šç”±è‡ªåŠ¨åŒ–è§„åˆ™ç”Ÿæˆï¼Œä¸æ„æˆä»»ä½•æŠ•èµ„å»ºè®®ã€‚")
    return "\n".join(report_parts)

def call_gemini_ai(prompt):
    try:
        genai.configure(api_key=os.environ.get("GEMINI_API_KEY"))
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content(prompt, request_options={'timeout': 180})
        return response.text
    except Exception as e:
        print(f"âŒ è°ƒç”¨ Gemini AI å¤±è´¥: {e}"); traceback.print_exc()
        return "AIæ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥æˆ–ç½‘ç»œè¿æ¥ã€‚"

def generate_ai_based_report(news, sectors, funds_string):
    print("æ­£åœ¨è¯·æ±‚â€œAIç­–ç•¥å¤§è„‘â€ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    if not funds_string.strip():
        return "ç”±äºæ‰€æœ‰æ•°æ®æºå‡æœªèƒ½è·å–ä»»ä½•åŸºé‡‘çš„è¯¦ç»†æ•°æ®ï¼ŒAIç­–ç•¥å¤§è„‘æ— æ³•è¿›è¡Œåˆ†æã€‚"
    analysis_prompt = f"ä½œä¸ºä¸€åæ•°æ®é©±åŠ¨çš„é‡åŒ–æŠ•èµ„ç­–ç•¥å¸ˆ...[çœç•¥è¯¦ç»†æŒ‡ä»¤]...\n**ç¬¬ä¸€éƒ¨åˆ†ï¼šå®è§‚æ–°é—»**\n{news}\n**ç¬¬äºŒéƒ¨åˆ†ï¼šæ¿å—è½®åŠ¨**\n{sectors}\n**ç¬¬ä¸‰éƒ¨åˆ†ï¼šæŒä»“åŸºé‡‘è¯¦ç»†æ•°æ®**\n{funds_string}"
    draft_article = call_gemini_ai(analysis_prompt)
    if "AIæ¨¡å‹è°ƒç”¨å¤±è´¥" in draft_article: return draft_article
    polish_prompt = f"ä½œä¸ºä¸€åå–„äºç”¨æ•°æ®è¯´è¯çš„æŠ•èµ„ç¤¾åŒºKOL...[çœç•¥è¯¦ç»†æŒ‡ä»¤]...\n**ã€åŸå§‹æŠ¥å‘Šã€‘**\n{draft_article}"
    return call_gemini_ai(polish_prompt)
    
def main():
    if not check_time_interval(): return
    config = load_config()
    beijing_time = datetime.datetime.utcnow() + datetime.timedelta(hours=8)
    
    raw_fund_datas = {}
    with concurrent.futures.ThreadPoolExecutor(max_workers=len(config['fund_codes'])) as executor:
        future_to_code = {executor.submit(get_fund_raw_data_final_robust, code, config['historical_days_to_fetch']): code for code in config['fund_codes']}
        for future in concurrent.futures.as_completed(future_to_code):
            code, raw_data = future_to_code[future], future.result()
            if raw_data: raw_fund_datas[code] = raw_data

    structured_fund_datas, formatted_fund_strings = [], []
    for code, raw_data in raw_fund_datas.items():
        structured, formatted = process_fund_data(raw_data, code, config['moving_average_days'], config['historical_days_to_display'])
        if structured and formatted:
            structured_fund_datas.append(structured)
            formatted_fund_strings.append(formatted)

    all_news_text = "\n".join([search_news(kw) for kw in config['news_keywords']])
    sector_data_text = get_sector_data()

    if not os.path.exists('reports'): os.makedirs('reports')
    
    rule_report = generate_rule_based_report(structured_fund_datas, beijing_time)
    rule_filename = f"reports/åŸºé‡‘åˆ†ææŠ¥å‘Š-è§„åˆ™ç‰ˆ_{beijing_time.strftime('%Y-%m-%d_%H-%M')}.md"
    with open(rule_filename, 'w', encoding='utf-8') as f: f.write(rule_report)
    print(f"\nâœ… â€œè§„åˆ™å¤§è„‘â€æŠ¥å‘Šå·²æˆåŠŸä¿å­˜ä¸º: {rule_filename}")

    ai_report = generate_ai_based_report(all_news_text, sector_data_text, "\n".join(formatted_fund_strings))
    ai_filename = f"reports/åŸºé‡‘åˆ†ææŠ¥å‘Š-AIç‰ˆ_{beijing_time.strftime('%Y-%m-%d_%H-%M')}.md"
    with open(ai_filename, 'w', encoding='utf-8') as f: f.write(ai_report)
    print(f"âœ… â€œAIç­–ç•¥å¤§è„‘â€æŠ¥å‘Šå·²æˆåŠŸä¿å­˜ä¸º: {ai_filename}")

    update_timestamp()

if __name__ == "__main__":
    main()
