# ================================================================
#                Project Prometheus - Final Production Version
#                     (AKShare Data Core & Chinese Edition)
# ================================================================
import os
import sys
import json
import yaml
import logging
from datetime import datetime, timedelta
import pytz
import pandas as pd
import akshare as ak  # å¼•å…¥ AKShare
import pandas_ta as ta
from fredapi import Fred
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
from concurrent.futures import ThreadPoolExecutor
from tenacity import retry, stop_after_attempt, wait_fixed
import google.generativeai as genai
import requests
from bs4 import BeautifulSoup
import google.api_core.exceptions

# --- Section 1: Setup & Configuration ---
try:
    with open('config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
except FileNotFoundError:
    print("FATAL: config.yaml not found. Exiting.")
    sys.exit(1)

# Logging Configuration
LOG_DIR = 'logs'
os.makedirs(LOG_DIR, exist_ok=True)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [%(levelname)s] - %(message)s',
                    handlers=[logging.FileHandler(os.path.join(LOG_DIR, 'workflow.log'), mode='w'),
                              logging.StreamHandler()])

# Matplotlib setup for non-GUI environment and Chinese characters
matplotlib.use('Agg')
matplotlib.rcParams['font.sans-serif'] = ['SimHei']
matplotlib.rcParams['axes.unicode_minus'] = False

# --- API Configuration ---
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
if not GEMINI_API_KEY:
    logging.error("FATAL: GEMINI_API_KEY environment variable not set.")
    sys.exit(1)
genai.configure(api_key=GEMINI_API_KEY)
AI_MODEL = genai.GenerativeModel('gemini-1.5-pro-latest')

# --- Section 2: Data Acquisition Layer (Now with AKShare) ---
@retry(stop=stop_after_attempt(3), wait=wait_fixed(3))
def fetch_url_content(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    response = requests.get(url, headers=headers, timeout=20)
    response.raise_for_status()
    return response.text

def scrape_news():
    headlines = []
    for source in config['data_sources']['news_urls']:
        try:
            logging.info(f"æ­£åœ¨ä» {source['name']} çˆ¬å–æ–°é—»...")
            html = fetch_url_content(source['url'])
            soup = BeautifulSoup(html, 'html.parser')
            links = soup.find_all('a', href=True)
            for link in links:
                if len(link.text.strip()) > 20 and '...' not in link.text:
                    headlines.append(link.text.strip())
            if not headlines:
                logging.warning(f"æœªèƒ½ä» {source['name']} æ‰¾åˆ°æœ‰æ•ˆæ–°é—»æ ‡é¢˜ã€‚")
        except Exception as e:
            logging.error(f"ä» {source['name']} çˆ¬å–æ–°é—»å¤±è´¥: {e}")
    return list(set(headlines))[:15]

# --- NEW: fetch_historical_data using AKShare ---
@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))
def fetch_historical_data_akshare(code, days):
    """
    ä½¿ç”¨AKShare APIè·å–åŸºé‡‘å†å²è¡Œæƒ…æ•°æ®ã€‚
    è¿™æ˜¯æ–°çš„ã€æ— éœ€Tokenã€æ›´å¯é çš„æ•°æ®æ ¸å¿ƒã€‚
    """
    logging.info(f"æ­£åœ¨ä½¿ç”¨AKShareè·å–åŸºé‡‘ {code} çš„å†å²æ•°æ®...")
    # AKShareä»ä¸œæ–¹è´¢å¯Œè·å–ETFå†å²æ•°æ®ï¼Œéå¸¸ç¨³å®š
    df = ak.fund_etf_hist_em(symbol=code, period="daily", adjust="qfq") # qfq = å‰å¤æƒ
    
    if df.empty:
        raise ValueError(f"AKShareæœªèƒ½è·å–åˆ°ä»£ç  {code} çš„æ•°æ®ã€‚è¯·æ£€æŸ¥ä»£ç æ˜¯å¦ä¸ºæœ‰æ•ˆçš„åœºå†…ETFä»£ç ã€‚")

    # æ•°æ®æ¸…æ´—å’Œæ ¼å¼ç»Ÿä¸€ï¼Œä»¥é€‚é…åç»­æ‰€æœ‰åˆ†ææ¨¡å—
    df = df.rename(columns={'æ—¥æœŸ': 'Date', 'å¼€ç›˜': 'Open', 'æ”¶ç›˜': 'Close', 'æœ€é«˜': 'High', 'æœ€ä½': 'Low', 'æˆäº¤é‡': 'Volume'})
    df['Date'] = pd.to_datetime(df['Date'])
    df = df.set_index('Date')
    
    # å°†æ‰€æœ‰ä»·æ ¼å’Œæˆäº¤é‡åˆ—è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼Œé˜²æ­¢åç»­è®¡ç®—å‡ºé”™
    cols_to_numeric = ['Open', 'Close', 'High', 'Low', 'Volume']
    df[cols_to_numeric] = df[cols_to_numeric].apply(pd.to_numeric, errors='coerce')
    df.dropna(subset=cols_to_numeric, inplace=True) # åˆ é™¤è½¬æ¢å¤±è´¥çš„è¡Œ

    # æŒ‰éœ€æˆªå–æŒ‡å®šå¤©æ•°çš„æ•°æ®
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)
    df = df[df.index >= start_date]

    df = df.sort_index() # ç¡®ä¿æ•°æ®æŒ‰æ—¶é—´å‡åºæ’åˆ—
    df['code'] = code
    return df

# ... (get_economic_data, performance review, and monte carlo functions remain the same)
def get_economic_data():
    if not config['economic_data']['enabled']: return "å®è§‚ç»æµæ•°æ®æ¨¡å—å·²ç¦ç”¨ã€‚"
    try:
        fred_key = os.getenv(config['economic_data']['fred_api_key_env'])
        if not fred_key: return "æœªèƒ½æ‰¾åˆ°FRED APIå¯†é’¥ç¯å¢ƒå˜é‡ã€‚"
        fred = Fred(api_key=fred_key)
        data_points = {}
        for indicator in config['economic_data']['indicators']:
            series = fred.get_series(indicator)
            if not series.empty:
                data_points[indicator] = f"{series.iloc[-1]} (æˆªè‡³ {series.index[-1].strftime('%Y-%m-%d')})"
        return f"æœ€æ–°å®è§‚ç»æµæŒ‡æ ‡: {json.dumps(data_points, indent=2, ensure_ascii=False)}"
    except Exception as e:
        logging.error(f"è·å–FREDæ•°æ®å¤±è´¥: {e}")
        return "æ— æ³•æ£€ç´¢å®è§‚ç»æµæ•°æ®ã€‚"

def evaluate_past_recommendations():
    return "ç»©æ•ˆè¯„ä¼°æ¨¡å—ç­‰å¾…å†å²æ•°æ®ç§¯ç´¯ã€‚"

def run_monte_carlo_simulation(all_fund_data):
    if not config['prometheus_module']['monte_carlo']['enabled']: return "è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿå·²ç¦ç”¨ã€‚", None
    if not all_fund_data or len(all_fund_data) < 2:
        logging.warning("å†å²æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿è¡Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿã€‚å·²è·³è¿‡ã€‚")
        return "è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿå·²è·³è¿‡ï¼šæœ‰æ•ˆçš„åŸºé‡‘æ•°æ®ä¸è¶³ã€‚", None
    try:
        logging.info("å¼€å§‹è¿›è¡Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ...")
        combined_data = pd.concat([df['Close'] for df in all_fund_data.values()], axis=1)
        combined_data.columns = list(all_fund_data.keys())
        daily_returns = combined_data.pct_change().dropna()
        if daily_returns.empty or len(daily_returns) < 2:
            logging.warning("åŸºé‡‘æ•°æ®æ— é‡å ï¼Œæ— æ³•è®¡ç®—åæ–¹å·®çŸ©é˜µã€‚å·²è·³è¿‡æ¨¡æ‹Ÿã€‚")
            return "è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿå·²è·³è¿‡ï¼šåŸºé‡‘æ•°æ®æ— é‡å éƒ¨åˆ†ã€‚", None
        mean_returns = daily_returns.mean()
        cov_matrix = daily_returns.cov()
        num_simulations, num_days = config['prometheus_module']['monte_carlo']['simulations'], config['prometheus_module']['monte_carlo']['projection_days']
        results = np.zeros((num_days, num_simulations))
        initial_portfolio_value = 100
        for i in range(num_simulations):
            daily_vol = np.random.multivariate_normal(mean_returns, cov_matrix, num_days)
            portfolio_daily_returns = daily_vol.mean(axis=1)
            path = np.zeros(num_days)
            path[0] = initial_portfolio_value * (1 + portfolio_daily_returns[0])
            for t in range(1, num_days):
                path[t] = path[t-1] * (1 + portfolio_daily_returns[t])
            results[:, i] = path
        plt.figure(figsize=(12, 7))
        plt.plot(results, alpha=0.1)
        plt.title(f'æŠ•èµ„ç»„åˆä»·å€¼é¢„æµ‹ ({num_simulations}æ¬¡æ¨¡æ‹Ÿ, æœªæ¥{num_days}å¤©)', fontsize=16)
        plt.xlabel('ä»ä»Šå¤©èµ·çš„äº¤æ˜“æ—¥', fontsize=12)
        plt.ylabel('æ ‡å‡†åŒ–çš„æŠ•èµ„ç»„åˆä»·å€¼', fontsize=12)
        plt.grid(True, linestyle='--', alpha=0.6)
        final_values = pd.Series(results[-1, :])
        percentiles = final_values.quantile([0.05, 0.50, 0.95])
        plt.axhline(y=percentiles[0.95], color='g', linestyle='--', label=f'95%ä¹è§‚æƒ…å†µ ({percentiles[0.95]:.2f})')
        plt.axhline(y=percentiles[0.50], color='b', linestyle='-', label=f'50%ä¸­æ€§æƒ…å†µ ({percentiles[0.50]:.2f})')
        plt.axhline(y=percentiles[0.05], color='r', linestyle='--', label=f'5%æ‚²è§‚æƒ…å†µ ({percentiles[0.05]:.2f})')
        plt.legend()
        chart_path = 'charts/monte_carlo_projection.png'
        plt.savefig(chart_path)
        plt.close()
        summary = (f"**è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿç»“æœ ({num_simulations}æ¬¡è·¯å¾„, {num_days}å¤©):**\n"
                   f"- **ä¹è§‚æƒ…å†µ (95åˆ†ä½):** æŠ•èµ„ç»„åˆä»·å€¼å¯èƒ½å¢é•¿è‡³ {percentiles[0.95]:.2f}ã€‚\n"
                   f"- **ä¸­æ€§é¢„æœŸ (50åˆ†ä½):** æŠ•èµ„ç»„åˆä»·å€¼é¢„æœŸåœ¨ {percentiles[0.50]:.2f} é™„è¿‘ã€‚\n"
                   f"- **æ‚²è§‚æƒ…å†µ (5åˆ†ä½):** æŠ•èµ„ç»„åˆä»·å€¼å¯èƒ½ä¸‹è·Œè‡³ {percentiles[0.05]:.2f}ã€‚")
        return summary, chart_path
    except Exception as e:
        logging.error(f"è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿå‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        return "è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿå› æ„å¤–é”™è¯¯æœªèƒ½è¿è¡Œã€‚", None

# --- Section 5: Ultimate AI Council (Chinese Prompt) ---
def ultimate_ai_council(context):
    logging.info("æ­£åœ¨å¬å¼€ç»ˆæAIå§”å‘˜ä¼š...")
    prompt = f"""
    æ‚¨æ˜¯â€œæ™®ç½—ç±³ä¿®æ–¯â€AIï¼Œä¸€ä¸ªç”±é¡¶çº§é‡‘èä¸“å®¶ç»„æˆçš„AIå§”å‘˜ä¼šã€‚æ‚¨çš„ä½¿å‘½æ˜¯æ ¹æ®æä¾›çš„æ‰€æœ‰æ•°æ®ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆä¸€ä»½æœºæ„çº§çš„ã€å®Œæ•´çš„ä¸­æ–‡æŠ•èµ„æŠ¥å‘Šã€‚
    **æ ¸å¿ƒç›®æ ‡:** ä¸ºç”¨æˆ·æä¾›ä¸€ä»½æ¸…æ™°ã€å¯æ‰§è¡Œã€ç†ç”±å……åˆ†çš„åˆåäº¤æ˜“æŠ•èµ„ç­–ç•¥ã€‚
    **ç”¨æˆ·ç”»åƒ:**
    - é£é™©åå¥½: {config['user_profile']['risk_profile']}
    - æŠ•èµ„å“²å­¦: "{config['user_profile']['investment_philosophy']}"
    **å½“å‰æŒä»“:**
    {json.dumps(context['portfolio'], indent=2, ensure_ascii=False)}
    **--- è¾“å…¥æ•°æ® ---**
    **1. è‡ªæˆ‘å­¦ä¹ ç»©æ•ˆè¯„ä¼° (æˆ‘è¿‡å»çš„å»ºè®®è¡¨ç°å¦‚ä½•ï¼Ÿ):**
    {context.get('performance_review', 'æš‚æ— ')}
    **2. å¸‚åœºæ–°é—»ä¸æƒ…ç»ª (å¸‚åœºæƒ…ç»ªå¦‚ä½•ï¼Ÿ):**
    {context['news'] if context.get('news') else 'æœªèƒ½è·å–åˆ°å¸‚åœºæ–°é—»ã€‚'}
    **3. å®è§‚ç»æµæ•°æ® (å®è§‚å¤§å±€æ˜¯æ€æ ·çš„ï¼Ÿ):**
    {context.get('economic_data', 'æš‚æ— ')}
    **4. é‡åŒ–åˆ†æ (æ•°æ®å’ŒæŒ‡æ ‡è¯´æ˜äº†ä»€ä¹ˆï¼Ÿ):**
    {context.get('quant_analysis', 'æœªèƒ½è·å–åˆ°ä»»ä½•åŸºé‡‘çš„é‡åŒ–æ•°æ®ã€‚')}
    **5. æœªæ¥é£é™©è¯„ä¼° (æ¦‚ç‡æ¨¡å‹é¢„æµ‹äº†ä»€ä¹ˆï¼Ÿ):**
    {context.get('monte_carlo_summary', 'æš‚æ— ')}
    **--- è¾“å‡ºæ ¼å¼è¦æ±‚ ---**
    æ‚¨å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç”Ÿæˆä¸¤éƒ¨åˆ†å†…å®¹ï¼Œå¹¶ç”¨ "---DETAILED_REPORT_CUT---" è¿™è¡Œæ–‡å­—ç²¾ç¡®åœ°åˆ†éš”å¼€ã€‚
    **ç¬¬ä¸€éƒ¨åˆ†: æ‰§è¡Œæ‘˜è¦ (ç”¨äºREADME.md)**
    # ğŸ”¥ æ™®ç½—ç±³ä¿®æ–¯æ¯æ—¥æŠ•èµ„ç®€æŠ¥
    **æŠ¥å‘Šæ—¶é—´:** {context['current_time']}
    **ä»Šæ—¥æ ¸å¿ƒè§‚ç‚¹:** (ç”¨ä¸€å¥è¯é«˜åº¦æ¦‚æ‹¬æ‚¨å¯¹ä»Šæ—¥å¸‚åœºçš„æ ¸å¿ƒåˆ¤æ–­)
    ---
    ### æŠ•èµ„ç»„åˆä»ªè¡¨ç›˜
    | åŸºé‡‘åç§° | ç±»å‹ | **æ“ä½œå»ºè®®** | **ä¿¡å¿ƒæŒ‡æ•°** | æ ¸å¿ƒç†ç”± |
    | :--- | :--- | :--- | :--- | :--- |
    (è¯·ä¸ºç”¨æˆ·çš„åŸºé‡‘æ± ä¸­çš„**æ¯ä¸€åªåŸºé‡‘**å¡«å……æ­¤è¡¨æ ¼ï¼Œæä¾›æ˜ç¡®çš„'æŒæœ‰', 'ä¹°å…¥', 'å‡ä»“', 'å–å‡º', 'è§‚æœ›'ç­‰å»ºè®®ï¼Œå¹¶ç»™å‡º'é«˜', 'ä¸­', 'ä½'çš„ä¿¡å¿ƒæŒ‡æ•°)
    ---
    ### æœªæ¥90å¤©è´¢å¯Œé¢„æµ‹ (è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ)
    ![æŠ•èµ„ç»„åˆé¢„æµ‹å›¾](charts/monte_carlo_projection.png)
    **é¦–å¸­é£é™©å®˜(CRO)çš„æœ€ç»ˆè£å†³:** (è§£è¯»è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿç»“æœã€‚ç»™å‡ºä¸€ä¸ªæ˜ç¡®çš„é£é™©ç­‰çº§ï¼šä½ã€ä¸­ã€é«˜ã€æˆ–æé«˜ï¼Œå¹¶è§£é‡ŠåŸå› ã€‚)
    ---
    *å…è´£å£°æ˜: æœ¬AIæŠ¥å‘Šç”±å…¬å¼€æ•°æ®è‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆä»»ä½•æŠ•èµ„å»ºè®®ã€‚æ‰€æœ‰é‡‘èå†³ç­–å‡åŒ…å«é£é™©ã€‚*
    ---DETAILED_REPORT_CUT---
    **ç¬¬äºŒéƒ¨åˆ†: æ·±åº¦åˆ†ææŠ¥å‘Š (ç”¨äº reports/report_YYYY-MM-DD.md)**
    # æ™®ç½—ç±³ä¿®æ–¯æ·±åº¦åˆ†ææŠ¥å‘Š - {context['current_date']}
    ## 1. é¦–å¸­æŠ•èµ„å®˜(CIO)å¼€ç¯‡é™ˆè¯
    (æä¾›ä¸€ä¸ªå…¨é¢çš„å¸‚åœºå®è§‚æ¦‚è¿°ï¼Œè§£é‡Šâ€œä»Šæ—¥æ ¸å¿ƒè§‚ç‚¹â€æ˜¯å¦‚ä½•å½¢æˆçš„ã€‚)
    ## 2. è‡ªæˆ‘å­¦ä¹ ä¸ç­–ç•¥è°ƒæ•´
    (è®¨è®ºç»©æ•ˆè¯„ä¼°æŠ¥å‘Šã€‚æ˜ç¡®è¯´æ˜è¿‡å»çš„æˆåŠŸæˆ–å¤±è´¥å¦‚ä½•å½±å“ä»Šå¤©çš„å»ºè®®ã€‚)
    ## 3. é€åªåŸºé‡‘æ·±åº¦å‰–æ
    (ä¸ºæ¯ä¸€åªåŸºé‡‘æä¾›æ•°æ®µåˆ†æï¼Œæ¶µç›–å®è§‚ã€é‡åŒ–ã€æƒ…ç»ªè§†è§’å’Œæœ€ç»ˆå†³ç­–é€»è¾‘ã€‚)
    ## 4. é£é™©è¯„ä¼°ä¸åº”æ€¥é¢„æ¡ˆ
    (è¯¦ç»†é˜è¿°CROçš„è£å†³ã€‚æŠ•èµ„ç»„åˆé¢ä¸´çš„ä¸»è¦é£é™©æ˜¯ä»€ä¹ˆï¼Ÿ)
    """
    try:
        logging.info("æ­£åœ¨ä½¿ç”¨ Gemini 1.5 Pro ç”Ÿæˆä¸­æ–‡æŠ¥å‘Š...")
        response = AI_MODEL.generate_content(prompt)
        report_text = response.text
        if "---DETAILED_REPORT_CUT---" in report_text:
            summary, detail = report_text.split("---DETAILED_REPORT_CUT---", 1)
        else:
            summary, detail = report_text, "AIæœªèƒ½ç”Ÿæˆç‹¬ç«‹çš„è¯¦ç»†æŠ¥å‘Šéƒ¨åˆ†ã€‚"
        return summary.strip(), detail.strip()
    except google.api_core.exceptions.ResourceExhausted as e:
        logging.error(f"AIæŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼ŒAPIé…é¢è€—å°½: {e}")
        return ("# ğŸ”¥ æ™®ç½—ç±³ä¿®æ–¯ç®€æŠ¥ç”Ÿæˆå¤±è´¥ï¼šAPIé…é¢è¶…é™\n\nå¯¹Gemini AIçš„è¯·æ±‚å·²è¢«æ‹’ç»ï¼Œå› ä¸ºå…è´¹å¥—é¤çš„APIé…é¢å·²ç”¨å®Œã€‚è¯·ä¸ºæ‚¨çš„Google Cloudé¡¹ç›®å¯ç”¨ç»“ç®—åŠŸèƒ½ã€‚å·¥ä½œæµå°†åœ¨æ˜å¤©é‡è¯•ã€‚", str(e))
    except Exception as e:
        logging.error(f"AIæŠ¥å‘Šç”Ÿæˆæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return ("# ğŸ”¥ æ™®ç½—ç±³ä¿®æ–¯ç®€æŠ¥ç”Ÿæˆå¤±è´¥\n\nç”ŸæˆAIæŠ¥å‘Šæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚", str(e))

# --- Section 6: Main Execution Block (Modified for AKShare) ---
def main():
    start_time = datetime.now(pytz.timezone('Asia/Shanghai'))
    logging.info(f"--- æ™®ç½—ç±³ä¿®æ–¯å¼•æ“å¯åŠ¨äº {start_time.strftime('%Y-%m-%d %H:%M:%S')} (AKShareæ ¸å¿ƒ) ---")
    
    context = {'current_time': start_time.strftime('%Y-%m-%d %H:%M:%S %Z'), 'current_date': start_time.strftime('%Y-%m-%d')}
    context['performance_review'] = evaluate_past_recommendations()

    with ThreadPoolExecutor(max_workers=10) as executor:
        news_future = executor.submit(scrape_news)
        eco_future = executor.submit(get_economic_data)
        
        fund_codes = [f['code'] for f in config['index_funds']]
        # --- MODIFIED: Use the new AKShare data fetching function ---
        hist_data_futures = {code: executor.submit(fetch_historical_data_akshare, code, 365) for code in fund_codes}

        context['news'] = "\n- ".join(news_future.result())
        context['economic_data'] = eco_future.result()
        
        all_fund_data = {}
        quant_reports = []
        for code in fund_codes:
            future = hist_data_futures[code]
            fund_name = next((f['name'] for f in config['index_funds'] if f['code'] == code), code)
            try:
                data = future.result()
                all_fund_data[code] = data
                data.ta.rsi(append=True)
                data.ta.macd(append=True)
                latest = data.iloc[-1]
                macd_signal = 'é‡‘å‰' if latest['MACD_12_26_9'] > latest['MACDs_12_26_9'] else 'æ­»å‰'
                quant_reports.append(f"  - **{fund_name} ({code})**: æ•°æ®æ­£å¸¸ã€‚RSI={latest['RSI_14']:.2f}, MACDä¿¡å·={macd_signal}")
            except Exception as e:
                logging.error(f"å¤„ç†åŸºé‡‘ {fund_name} ({code}) çš„æ•°æ®å¤±è´¥: {e}")
                quant_reports.append(f"  - **{fund_name} ({code})**: æ•°æ®è·å–å¤±è´¥ã€‚è¯·æ£€æŸ¥åŸºé‡‘ä»£ç æ˜¯å¦æ­£ç¡®ã€‚")
        
        context['quant_analysis'] = "æœ€æ–°æŠ€æœ¯æŒ‡æ ‡åˆ†æ:\n" + "\n".join(quant_reports)

    try:
        with open(config['user_profile']['portfolio_path'], 'r', encoding='utf-8') as f:
            context['portfolio'] = json.load(f)
    except Exception as e:
        logging.error(f"æ— æ³•åŠ è½½æŒä»“æ–‡ä»¶: {e}")
        context['portfolio'] = [{"é”™è¯¯": "æ— æ³•åŠ è½½æŒä»“æ–‡ä»¶ã€‚"}]

    context['monte_carlo_summary'], _ = run_monte_carlo_simulation(all_fund_data)
    
    if not all_fund_data:
        logging.warning("è·³è¿‡AIå§”å‘˜ä¼šï¼šæœªèƒ½è·å–åˆ°ä»»ä½•æœ‰æ•ˆçš„åŸºé‡‘æ•°æ®ã€‚")
        summary_report, detail_report = (f"# ğŸ”¥ æ™®ç½—ç±³ä¿®æ–¯ç®€æŠ¥ç”Ÿæˆå¤±è´¥ï¼šæ— æœ‰æ•ˆæ•°æ®\n\næ‰€æœ‰ç›®æ ‡åŸºé‡‘çš„æ•°æ®è·å–å‡å¤±è´¥ã€‚è¯·æ£€æŸ¥åŸºé‡‘ä»£ç æ˜¯å¦ä¸ºæœ‰æ•ˆçš„åœºå†…ETFã€‚ç³»ç»Ÿå°†åœ¨ä¸‹ä¸ªè®¡åˆ’æ—¶é—´è‡ªåŠ¨é‡è¯•ã€‚", 
                                         "æ‰€æœ‰æ•°æ®è·å–ä»»åŠ¡å‡å¤±è´¥ã€‚è¯·æ£€æŸ¥æ—¥å¿—ä¸­å…³äºAKShareçš„è¯¦ç»†é”™è¯¯ä¿¡æ¯ã€‚")
    else:
        summary_report, detail_report = ultimate_ai_council(context)

    with open("README.md", "w", encoding="utf-8") as f: f.write(summary_report)
    
    report_filename = f"reports/report_{context['current_date']}.md"
    os.makedirs('reports', exist_ok=True)
    with open(report_filename, "w", encoding="utf-8") as f: f.write(detail_report)

    end_time = datetime.now(pytz.timezone('Asia/Shanghai'))
    logging.info(f"--- æ™®ç½—ç±³ä¿®æ–¯å¼•æ“ç»“æŸäº {end_time.strftime('%Y-%m-%d %H:%M:%S')} ---")
    logging.info(f"--- æ€»è¿è¡Œæ—¶é—´: {end_time - start_time} ---")

if __name__ == "__main__":
    main()
