import os
import datetime
import time
import json
import concurrent.futures
import google.generativeai as genai
import requests
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS

# --- å…¨å±€è®¾ç½® ---
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Referer': 'http://fund.eastmoney.com/'
}
TIMESTAMP_FILE = "last_run_timestamp.txt"
MIN_INTERVAL_HOURS = 6
REQUESTS_TIMEOUT = 20

# --- åŠŸèƒ½å‡½æ•° ---

def load_config():
    """ä»config.jsonåŠ è½½é…ç½®"""
    print("æ­£åœ¨åŠ è½½é…ç½®æ–‡ä»¶ config.json...")
    with open('config.json', 'r', encoding='utf-8') as f:
        config = json.load(f)
    # ä¸ºé…ç½®æ–‡ä»¶å¢åŠ é»˜è®¤å€¼ï¼Œé˜²æ­¢ç”¨æˆ·å¿˜è®°å¡«å†™
    config.setdefault('historical_days_to_display', 7)
    return config

def check_time_interval():
    """æ£€æŸ¥çªå‘äº‹ä»¶è§¦å‘é—´éš”"""
    github_event_name = os.getenv('GITHUB_EVENT_NAME')
    if github_event_name != 'repository_dispatch': return True
    if not os.path.exists(TIMESTAMP_FILE): return True
    with open(TIMESTAMP_FILE, "r") as f: last_run_timestamp = float(f.read())
    current_timestamp = time.time()
    hours_diff = (current_timestamp - last_run_timestamp) / 3600
    if hours_diff < MIN_INTERVAL_HOURS:
        print(f"è·ç¦»ä¸Šæ¬¡æ‰§è¡Œï¼ˆ{hours_diff:.2f}å°æ—¶ï¼‰æœªè¶…è¿‡{MIN_INTERVAL_HOURS}å°æ—¶ï¼Œæœ¬æ¬¡è·³è¿‡ã€‚")
        return False
    return True

def update_timestamp():
    """æ›´æ–°æ—¶é—´æˆ³æ–‡ä»¶"""
    with open(TIMESTAMP_FILE, "w") as f: f.write(str(time.time()))

def search_news(keyword):
    """æœç´¢æ–°é—»"""
    print(f"æ­£åœ¨æœç´¢æ–°é—»: {keyword}...")
    try:
        with DDGS() as ddgs:
            results = [f"- [æ ‡é¢˜] {r['title']}\n  [æ‘˜è¦] {r.get('body', 'æ— ')}\n" for r in ddgs.news(keyword, region='cn-zh', safesearch='off', max_results=5)]
            return "\n".join(results)
    except Exception as e: return f"æœç´¢å…³é”®è¯ '{keyword}' å¤±è´¥: {e}"

def get_sector_data():
    """çˆ¬å–è¡Œä¸šæ¿å—æ•°æ®"""
    print("æ­£åœ¨çˆ¬å–è¡Œä¸šæ¿å—æ•°æ®...")
    url = "http://quote.eastmoney.com/center/boardlist.html#industry_board"
    try:
        response = requests.get(url, headers=HEADERS, timeout=REQUESTS_TIMEOUT)
        soup = BeautifulSoup(response.text, 'html.parser')
        sectors = []
        for row in soup.select('table#table_wrapper-table tbody tr'):
            cols = row.find_all('td')
            if len(cols) > 5:
                try:
                    name = cols[1].find('a').text.strip()
                    change = float(cols[4].text.strip().replace('%', ''))
                    sectors.append({'name': name, 'change': change})
                except (ValueError, AttributeError): continue
        
        sectors.sort(key=lambda x: x['change'], reverse=True)
        top_rising = sectors[:10]; top_falling = sectors[-10:]
        rising_str = "\n".join([f"  - {s['name']}: {s['change']:.2f}%" for s in top_rising])
        falling_str = "\n".join([f"  - {s['name']}: {s['change']:.2f}%" for s in top_falling])
        return f"**ã€çƒ­é—¨ä¸Šæ¶¨æ¿å—ã€‘**\n{rising_str}\n\n**ã€çƒ­é—¨ä¸‹è·Œæ¿å—ã€‘**\n{falling_str}"
    except Exception as e: return f"è¡Œä¸šæ¿å—æ•°æ®çˆ¬å–å¤±è´¥: {e}"

def get_fund_data_with_details(fund_code, history_days, ma_days, days_to_display):
    """è·å–åŸºé‡‘è¯¦ç»†æ•°æ®ï¼ŒåŒ…æ‹¬å†å²æ•°æ®è¡¨å’ŒæŠ€æœ¯åˆ†æ"""
    print(f"æ­£åœ¨è·å–åŸºé‡‘ {fund_code} çš„è¯¦ç»†æ•°æ®...")
    try:
        url = f"http://api.fund.eastmoney.com/f10/lsjz?fundCode={fund_code}&pageIndex=1&pageSize={history_days}"
        response = requests.get(url, headers=HEADERS, timeout=REQUESTS_TIMEOUT).json()
        
        fund_name = response['Data']['FundBaseInfo']['JJJC']
        data = response['Data']['LSJZList']
        
        df = pd.DataFrame(data)
        df['FSRQ'] = pd.to_datetime(df['FSRQ'])
        df['DWJZ'] = pd.to_numeric(df['DWJZ'])
        df = df.sort_values('FSRQ')
        
        df[f'MA{ma_days}'] = df['DWJZ'].rolling(window=ma_days).mean()
        
        latest_data = df.iloc[-1]
        latest_price = latest_data['DWJZ']
        latest_ma = latest_data[f'MA{ma_days}']
        
        recent_df = df.tail(days_to_display).sort_values('FSRQ', ascending=False)
        table_header = f"| æ—¥æœŸ       | å•ä½å‡€å€¼ | {ma_days}æ—¥å‡çº¿ | è¶‹åŠ¿ |"
        table_divider = "|:-----------|:---------|:------------|:-----|"
        table_rows = []
        for _, row in recent_df.iterrows():
            ma_val = row[f'MA{ma_days}']
            ma_str = f"{ma_val:.4f}" if not pd.isna(ma_val) else "N/A"
            trend_emoji = "ğŸ“ˆ" if row['DWJZ'] > ma_val else "ğŸ“‰" if not pd.isna(ma_val) else "ğŸ¤”"
            table_rows.append(f"| {row['FSRQ'].strftime('%Y-%m-%d')} | {row['DWJZ']:.4f}   | {ma_str}    | {trend_emoji}  |")
        historical_table = "\n".join([table_header, table_divider] + table_rows)
        
        return f"""
### åŸºé‡‘: {fund_name} ({fund_code})
- **æœ€æ–°å‡€å€¼**: {latest_price:.4f} (æ—¥æœŸ: {latest_data['FSRQ'].strftime('%Y-%m-%d')})
- **{ma_days}æ—¥å‡çº¿**: {latest_ma:.4f if not pd.isna(latest_ma) else 'æ•°æ®ä¸è¶³'}
- **æŠ€æœ¯åˆ†æ**: å½“å‰å‡€å€¼åœ¨ {ma_days}æ—¥å‡çº¿**{'ä¹‹ä¸Š' if latest_price > latest_ma else 'ä¹‹ä¸‹'}**ï¼Œè¡¨æ˜çŸ­æœŸè¶‹åŠ¿å¯èƒ½**{'åå¼º' if latest_price > latest_ma else 'åå¼±'}**ã€‚
- **æœ€è¿‘ {days_to_display} æ—¥è¯¦ç»†æ•°æ®**:
{historical_table}
"""
    except Exception as e:
        return f"\n### åŸºé‡‘: {fund_code}\n- è¯¦ç»†æ•°æ®è·å–å¤±è´¥: {e}\n"

def call_gemini_ai(prompt):
    """è°ƒç”¨Gemini AI"""
    print("æ­£åœ¨è°ƒç”¨ Gemini AI è¿›è¡Œæ·±åº¦åˆ†æ...")
    try:
        genai.configure(api_key=os.environ.get("GEMINI_API_KEY"))
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content(prompt)
        return response.text
    except Exception as e: return f"AIæ¨¡å‹è°ƒç”¨å¤±è´¥: {e}"

def main():
    if not check_time_interval(): return
    config = load_config()
    
    print("å¼€å§‹æ‰§è¡Œâ€œæ•°æ®é©±åŠ¨ç‰ˆâ€åŸºé‡‘åˆ†æå·¥ä½œæµ...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
        news_futures = {executor.submit(search_news, kw): kw for kw in config['news_keywords']}
        fund_futures = {executor.submit(get_fund_data_with_details, code, config['historical_days_to_fetch'], config['moving_average_days'], config['historical_days_to_display']): code for code in config['fund_codes']}
        sector_future = executor.submit(get_sector_data)

        all_news_text = "\n".join([f.result() for f in concurrent.futures.as_completed(news_futures)])
        all_fund_data = "\n".join([f.result() for f in concurrent.futures.as_completed(fund_futures)])
        sector_data_text = sector_future.result()

    print("\n" + "="*50 + "\n--- åŸå§‹æ•°æ®æ±‡æ€» ---\n" + "="*50)
    print(f"\n--- å®è§‚æ–°é—» ---\n{all_news_text}")
    print(f"\n--- è¡Œä¸šæ¿å— ---\n{sector_data_text}")
    print(f"\n--- åŸºé‡‘è¯¦ç»†æ•°æ®ä¸åˆ†æ ---\n{all_fund_data}")
    print("\n" + "="*50)

    analysis_prompt = f"""
ä½œä¸ºä¸€åæ•°æ®é©±åŠ¨çš„é‡åŒ–æŠ•èµ„ç­–ç•¥å¸ˆï¼Œè¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹ä¸‰æ–¹é¢ä¿¡æ¯ï¼Œæ’°å†™ä¸€ä»½é€»è¾‘ä¸¥å¯†ã€æœ‰æ•°æ®æ”¯æ’‘çš„æŠ•èµ„ç­–ç•¥æŠ¥å‘Šã€‚

**ç¬¬ä¸€éƒ¨åˆ†ï¼šå®è§‚æ–°é—» (å¸‚åœºæƒ…ç»ª)**
{all_news_text}

**ç¬¬äºŒéƒ¨åˆ†ï¼šæ¿å—è½®åŠ¨ (å¸‚åœºç»“æ„)**
{sector_data_text}

**ç¬¬ä¸‰éƒ¨åˆ†ï¼šæŒä»“åŸºé‡‘çš„è¯¦ç»†æ•°æ® (é‡åŒ–äº‹å®)**
{all_fund_data}

**ã€ç­–ç•¥æŠ¥å‘Šæ’°å†™æŒ‡ä»¤ã€‘**
1.  **å¸‚åœºå®šè°ƒ**: ç»“åˆã€å®è§‚æ–°é—»ã€‘å’Œã€æ¿å—æ•°æ®ã€‘ï¼Œå¯¹å½“å‰å¸‚åœºç¯å¢ƒï¼ˆè¿›æ”»/é˜²å®ˆï¼‰å’Œä¸»çº¿çƒ­ç‚¹åšå‡ºåˆ¤æ–­ã€‚
2.  **æ•°æ®è§£è¯»**: é€ä¸€åˆ†æã€æŒä»“åŸºé‡‘ã€‘ã€‚**ä½ çš„åˆ†æå¿…é¡»åŸºäºâ€œæœ€è¿‘å‡ æ—¥è¯¦ç»†æ•°æ®â€è¡¨æ ¼**ã€‚
    *   æ˜ç¡®æŒ‡å‡ºå‡€å€¼çš„**è¿ç»­å˜åŒ–è¶‹åŠ¿**ã€‚ä¾‹å¦‚ï¼šâ€œä»æ•°æ®è¡¨å¯ä»¥çœ‹å‡ºï¼Œè¯¥åŸºé‡‘å‡€å€¼å·²è¿ç»­ä¸‰æ—¥ä¸‹è·Œï¼Œä»Xæ—¥çš„Aå…ƒè·Œè‡³Yæ—¥çš„Bå…ƒã€‚â€
    *   å°†å‡€å€¼ä¸å‡çº¿è¿›è¡Œ**åŠ¨æ€æ¯”è¾ƒ**ã€‚ä¾‹å¦‚ï¼šâ€œåœ¨Zæ—¥ï¼Œè¯¥åŸºé‡‘å‡€å€¼è·Œç ´äº†20æ—¥å‡çº¿ï¼Œè¿™æ˜¯ä¸€ä¸ªæ˜ç¡®çš„çŸ­æœŸèµ°å¼±ä¿¡å·ã€‚â€
3.  **ç­–ç•¥åˆ¶å®š (å¿…é¡»å¼•ç”¨æ•°æ®)**:
    *   **åŠ ä»“å»ºè®®**: å¿…é¡»è¯´æ˜ç†ç”±ï¼Œä¾‹å¦‚ï¼šâ€œå»ºè®®åŠ ä»“XXåŸºé‡‘ï¼Œå› ä¸ºå…¶é‡ä»“çš„YYæ¿å—å¤„äºä¸Šæ¶¨è¶‹åŠ¿ï¼Œå¹¶ä¸”å…¶å‡€å€¼å·²è¿ç»­Næ—¥ç«™åœ¨å‡çº¿ä¸Šæ–¹ï¼Œè¡¨ç°å¼ºåŠ¿ã€‚â€
    *   **å‡ä»“å»ºè®®**: å¿…é¡»è¯´æ˜ç†ç”±ï¼Œä¾‹å¦‚ï¼šâ€œå»ºè®®å‡ä»“XXåŸºé‡‘ï¼Œæ•°æ®æ˜¾ç¤ºå…¶å‡€å€¼åœ¨[æ—¥æœŸ]å·²è·Œç ´å…³é”®å‡çº¿ï¼Œä¸”è‡³ä»Šæœªèƒ½æ”¶å¤ï¼Œä¸‹è¡Œé£é™©è¾ƒå¤§ã€‚â€
4.  **æ€»ç»“ä¸é£é™©**: æ€»ç»“æ ¸å¿ƒæ“ä½œï¼Œå¹¶æç¤ºé£é™©ã€‚
"""
    draft_article = call_gemini_ai(analysis_prompt)
    
    polish_prompt = f"""
ä½œä¸ºä¸€åå–„äºç”¨æ•°æ®è¯´è¯çš„æŠ•èµ„ç¤¾åŒºKOLï¼Œè¯·å°†ä»¥ä¸‹è¿™ä»½ä¸“ä¸šçš„æŠ•ç ”æŠ¥å‘Šï¼Œè½¬åŒ–ä¸ºä¸€ç¯‡å¯¹æ™®é€šæŠ•èµ„è€…æå…·å¸å¼•åŠ›å’Œè¯´æœåŠ›çš„æ–‡ç« ã€‚

**ã€åŸå§‹æŠ¥å‘Šã€‘**
{draft_article}

**ã€æ¶¦è‰²è¦æ±‚ã€‘**
1.  **æ ‡é¢˜**: è¦æœ‰å†²å‡»åŠ›ï¼Œçªå‡ºâ€œæ•°æ®â€å’Œâ€œçœŸç›¸â€ï¼Œä¾‹å¦‚ï¼šâ€œåˆ«å…‰å¬æ¶ˆæ¯ï¼çœŸé‡‘ç™½é“¶çš„æ•°æ®å‘Šè¯‰ä½ ï¼ŒåŸºé‡‘æ˜¯è¯¥èµ°æ˜¯è¯¥ç•™ï¼Ÿâ€
2.  **æ ¸å¿ƒäº®ç‚¹**: åœ¨æ–‡ç« å¼€å¤´ï¼Œå°±å‘Šè¯‰è¯»è€…ï¼Œæœ¬æ–‡æœ€å¤§çš„ä¸åŒæ˜¯â€œç”¨æ•°æ®è¯´è¯â€ï¼Œä¼šå±•ç¤ºæ¯æ”¯åŸºé‡‘æœ€è¿‘ä¸€å‘¨çš„â€œæˆç»©å•â€ã€‚
3.  **æ•°æ®è¡¨æ ¼å¯è§†åŒ–**: å°†æŠ¥å‘Šä¸­çš„æ•°æ®è¡¨æ ¼ç¾åŒ–ã€‚åœ¨è¡¨æ ¼ä¸Šæ–¹åŠ ä¸Šç±»ä¼¼â€œè¯ä¸å¤šè¯´ï¼Œç›´æ¥ä¸Šæ•°æ®â€çš„å¼•å¯¼è¯­ï¼Œè®©è¡¨æ ¼æˆä¸ºæ–‡ç« çš„æ ¸å¿ƒè¯æ®ã€‚
4.  **è§£è¯»è¦é€šä¿—**: æŠŠâ€œè·Œç ´å‡çº¿â€è§£é‡Šä¸ºâ€œçŸ­æœŸåŠ¿å¤´ä¸å¯¹ï¼Œè¦å°å¿ƒäº†â€ï¼ŒæŠŠâ€œç«™ä¸Šå‡çº¿â€è§£é‡Šä¸ºâ€œçŠ¶æ€å›æš–ï¼Œå¯ä»¥å¤šçœ‹ä¸¤çœ¼â€ã€‚
5.  **ç»“è®ºè¦æ¸…æ™°**: åœ¨æ–‡ç« ç»“å°¾ï¼Œç”¨1ã€2ã€3ç‚¹æ¸…æ™°åœ°åˆ—å‡ºæ“ä½œå»ºè®®æ€»ç»“ã€‚
6.  **ç»“å°¾**: å¼ºåŠ¿æ€»ç»“è§‚ç‚¹ï¼Œå¹¶é™„ä¸Šå…è´£å£°æ˜ã€‚
"""
    final_article = call_gemini_ai(polish_prompt)

    print("\n--- æœ€ç»ˆç”Ÿæˆçš„ç¤¾åŒºæ–‡ç«  ---")
    print(final_article)
    if not os.path.exists('reports'): os.makedirs('reports')
    beijing_time = datetime.datetime.utcnow() + datetime.timedelta(hours=8)
    file_name = f"reports/åŸºé‡‘åˆ†ææŠ¥å‘Š_{beijing_time.strftime('%Y-%m-%d_%H-%M')}.md"
    with open(file_name, 'w', encoding='utf-8') as f: f.write(final_article)
    print(f"\næŠ¥å‘Šå·²æˆåŠŸä¿å­˜ä¸º: {file_name}")
    
    update_timestamp()

if __name__ == "__main__":
    main()
